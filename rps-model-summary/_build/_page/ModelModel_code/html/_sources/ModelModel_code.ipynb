{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: Model Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a) human_reward_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=os.path.join('data','rps_v2_clean.csv')\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:8951e2981df4ad656f4e9f0d8a3189cd298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 130738272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:8951e2981df4ad656f4e9f0d8a3189cd298...\n",
       "1                                     size 130738272"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = read_rps_data()\n",
    "# df.head()\n",
    "\n",
    "## add new columns to count the number of each reward\n",
    "new_df = add_col(df, ['rock_reward', 'paper_reward','scissors_reward',], value =0)\n",
    "new_df\n",
    "\n",
    "df = add_col(new_df, ['opponent_move'], value ='')\n",
    "def get_opponent_move(sub_df):\n",
    "    \"\"\"\n",
    "    fills in the `opponent_move` column\n",
    "    \"\"\"\n",
    "    for i in range(len(sub_df)):\n",
    "        if i%2 == 0:\n",
    "            sub_df.at[i, 'opponent_move'] = sub_df.at[i + 1, 'player_move']\n",
    "        else:\n",
    "            sub_df.at[i, 'opponent_move'] = sub_df.at[i - 1, 'player_move']\n",
    "    \n",
    "    return sub_df\n",
    "\n",
    "separated = separate_df(df)\n",
    "for e in separated:\n",
    "    get_opponent_move(e)\n",
    "df = pd.concat(separated)\n",
    "df\n",
    "\n",
    "# 3 points for a win, 0 for a tie, -1 for a loss\n",
    "def human_reward_move(sub_df):\n",
    "\n",
    "    ## number of reward points for each move\n",
    "    dic_reward={'win':3,'tie':0,'loss':-1}\n",
    "\n",
    "    ## initialization for each move\n",
    "    dic_move={'rock':0,'paper':0,'scissors':0}\n",
    "\n",
    "    ## loop through the human rows\n",
    "    for i in range(0,len(sub_df),2):\n",
    "\n",
    "        ## fetch player outcomes\n",
    "        outcome=sub_df.get('player_outcome').iloc[i]\n",
    "\n",
    "        ## fetch player moves\n",
    "        move=sub_df.get('player_move').iloc[i]\n",
    "\n",
    "        ## avoid all the nans and 'none's in player_move\n",
    "        if move != 'none'and not pd.isna(move): \n",
    "\n",
    "            ## tally all the point associated with rewards\n",
    "            dic_move[move]+=dic_reward[outcome]\n",
    "\n",
    "            ## add columns to store reward points\n",
    "            col_name=move+'_reward'\n",
    "\n",
    "            ## store reward points in corresponding columns\n",
    "            sub_df.at[i,'rock_reward']=dic_move['rock']\n",
    "            sub_df.at[i,'paper_reward']=dic_move['paper']\n",
    "            sub_df.at[i,'scissors_reward']=dic_move['scissors']\n",
    "\n",
    "    return sub_df \n",
    "\n",
    "separated = separate_df(df)\n",
    "for e in separated:\n",
    "    human_reward_move(e)\n",
    "r = pd.concat(separated)\n",
    "df=r\n",
    "df\n",
    "\n",
    "def move_matrix():\n",
    "        outcomes = pd.DataFrame.from_dict({\n",
    "        'win':     ['0', '1', '-1'],\n",
    "        'stay':    ['-1', '0', '1'],\n",
    "        'loss': ['1', '-1', '0']\n",
    "        }, orient='index', columns=['win', 'paper', 'scissors'])\n",
    "        outcomes = np.transpose(outcomes)\n",
    "        return outcomes\n",
    "comp_matrix = move_matrix()\n",
    "\n",
    "def softmax(x, beta = 1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x * beta) / np.sum(np.exp(x * beta), axis=0)\n",
    "\n",
    "\n",
    "def get_softmax_probabilities(df, columns):\n",
    "    distribution = []\n",
    "    vals = df[columns]\n",
    "    for i in range(df.shape[0]):\n",
    "        soft_max = softmax(vals.iloc[i], beta = 4).tolist() # add aggressive beta term to make max probability move more likely\n",
    "        distribution.append(soft_max)\n",
    "    dist = np.array(distribution)\n",
    "    sofm = pd.DataFrame(dist, columns = ['softmax_prob_rock', 'softmax_prob_paper', 'softmax_prob_scissors'])\n",
    "    \n",
    "    return sofm\n",
    "\n",
    "\n",
    "def pick_move(df, sofm):\n",
    "    moves = np.array([])\n",
    "    for i in range(df.shape[0]):\n",
    "        move_choices = ['rock', 'paper', 'scissors']\n",
    "        distribution = sofm.iloc[i].tolist() # get ith [rock_prob,paper_prob,scissors_prob] from input df \n",
    "        # https://www.w3schools.com/python/ref_random_choices.asp\n",
    "        # can also use other sample function\n",
    "        chosen_move = random.choices(move_choices, distribution) \n",
    "        moves = np.append(moves, chosen_move)\n",
    "    df = df.assign(agent_move = moves) # agent_move stores sampled moves\n",
    "    return df\n",
    "\n",
    "# from utils.py\n",
    "OUTCOME_LOOKUP = np.transpose(\n",
    "    pd.DataFrame.from_dict({\n",
    "        'rock':     ['tie', 'lose', 'win'],\n",
    "        'paper':    ['win', 'tie', 'lose'],\n",
    "        'scissors': ['lose', 'win', 'tie']\n",
    "    }, orient='index', columns=['rock', 'paper', 'scissors']))\n",
    "\n",
    "def evaluate_outcome(player_move, opponent_move):\n",
    "    \"\"\"\n",
    "    TODO check that both moves are in outcome lookup\n",
    "    \"\"\"\n",
    "    OUTCOME_LOOKUP = np.transpose(\n",
    "    pd.DataFrame.from_dict({\n",
    "        'rock':     ['tie', 'lose', 'win'],\n",
    "        'paper':    ['win', 'tie', 'lose'],\n",
    "        'scissors': ['lose', 'win', 'tie']\n",
    "    }, orient='index', columns=['rock', 'paper', 'scissors']))\n",
    "    \n",
    "    return OUTCOME_LOOKUP[player_move][opponent_move]\n",
    "\n",
    "\n",
    "def assign_agent_outcomes(df):\n",
    "    \"\"\"\n",
    "    Assign outcomes for the agent based on agent move choices.\n",
    "    df should include only human rows, since agent outcomes are irrelevant for simulating bots\n",
    "    \"\"\"\n",
    "    # df.assign(agent_outcome = '')\n",
    "    df['agent_outcome'] = df.apply(lambda x: evaluate_outcome(x['agent_move'], x['opponent_move']), axis=1)\n",
    "    return df\n",
    "\n",
    "# here, 2, 3, 1 would be the values in `rock_reward`, `paper_reward`, `scissors_reward`\n",
    "softmax([2, 3, 1])\n",
    "\n",
    "# agent_move column stores simulated move\n",
    "# agent_outcome stores outcome of simulated agent move and opponent move\n",
    "df=add_col(df,['agent_move','agent_outcome'],value=\"\")\n",
    "\n",
    "# code from the original model: \n",
    "soft_dist = get_softmax_probabilities(\n",
    "    df, # df should be just human rows at this point, strip out nans etc. \n",
    "    ['rock_reward', 'paper_reward', 'scissors_reward']\n",
    ")\n",
    "\n",
    "\n",
    "# Select agent move based on softmax computed above (1 min.)\n",
    "'''\n",
    "ONLY want to pick move for human row right? generate agent_outcome for human rows?\n",
    "so need a for i in range(2,len(df),2)\n",
    "'''\n",
    "df = pick_move(df, soft_dist) \n",
    "\n",
    "# # Evaluate outcome of agent move choices in simulation above\n",
    "# df = assign_agent_outcomes(df)\n",
    "\n",
    "def assign_agent_outcomes(df):\n",
    "    \"\"\"\n",
    "    Assign outcomes for the agent based on agent move choices.\n",
    "    df should include only human rows, since agent outcomes are irrelevant for simulating bots\n",
    "    \"\"\"\n",
    "    # df.assign(agent_outcome = '')\n",
    "    df['agent_outcome'] = df.apply(lambda x: evaluate_outcome(x['agent_move'], x['opponent_move']), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_3a=df[df['is_bot']!=1]\n",
    "\n",
    "df_3a['agent_outcome']=df_3a.apply(lambda x: evaluate_outcome(x['agent_move'],x['opponent_move']), axis=1)\n",
    "\n",
    "N_ROUNDS = 300\n",
    "def groupby_f_data(f_data, colname, bins):\n",
    "    \"\"\"\n",
    "    group by filtered data with player outcome and calculate the win percentage\n",
    "    colname will be either 'player_outcome' or 'agent_outcome' for plotting human or agent results\n",
    "    \"\"\"\n",
    "    modified_f_data = f_data.dropna()\n",
    "    labs = [str(int(round(a * (N_ROUNDS / bins), 0))) for a in range(1, bins + 1)]\n",
    "    modified_f_data['bin'] = pd.cut(modified_f_data.loc[:, ('round_index')], bins, labels = labs)\n",
    "    grouped_data = modified_f_data[['bot_strategy', 'player_id','bin', colname]].groupby(\n",
    "        ['bot_strategy', 'player_id', 'bin'])[colname].value_counts('count').rename('pct').reset_index()\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "def win_summary(grouped_data, colname):\n",
    "    \"\"\"\n",
    "    filter out the win data and add mean, SD, and SEM\n",
    "    colname will be either 'player_outcome' or 'agent_outcome' for plotting human or agent results\n",
    "    \"\"\"\n",
    "    win_data = grouped_data[grouped_data[colname] == 'win'].reset_index()\n",
    "    win_summary = win_data[['bot_strategy', 'bin', 'pct']].groupby(\n",
    "        ['bot_strategy', 'bin'])['pct'].agg(\n",
    "            [np.mean, np.std, stats.sem]).reset_index()\n",
    "    \n",
    "    return win_summary\n",
    "\n",
    "\n",
    "def plot_win_rates(data):\n",
    "    \"\"\"\n",
    "    generate plot displaying win rates against each bot, binned by rounds\n",
    "    \"\"\"\n",
    "    sns.set_style(style='white')\n",
    "    data['bot_strategy'] = data['bot_strategy'].replace([\n",
    "        'prev_move_positive', 'prev_move_negative', \n",
    "        'opponent_prev_move_positive', 'opponent_prev_move_nil',\n",
    "        'win_nil_lose_positive', 'win_positive_lose_negative',\n",
    "        'outcome_transition_dual_dependency'\n",
    "    ],\n",
    "    [\n",
    "        'Previous move (+)', 'Previous move (-)',\n",
    "        'Opponent previous move (+)', 'Opponent previous move (0)',\n",
    "        'Win-stay-lose-positive', 'Win-positive-lose-negative',\n",
    "        'Outcome-transition dual dependency'\n",
    "    ])\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(15, 10))\n",
    "    g = sns.scatterplot(\n",
    "        x = \"bin\", y = \"mean\", hue = \"bot_strategy\", \n",
    "        hue_order = [\n",
    "            'Previous move (+)', 'Previous move (-)',\n",
    "            'Opponent previous move (+)', 'Opponent previous move (0)',\n",
    "            'Win-stay-lose-positive', 'Win-positive-lose-negative',\n",
    "            'Outcome-transition dual dependency'\n",
    "        ],\n",
    "        palette=\"deep\", s = 200, ax = ax, data = data)\n",
    "    \n",
    "    plt.errorbar(data.get('bin'), data.get('mean'), yerr = data.get('sem'), \n",
    "        fmt = '.', ecolor='0.5', color='0.5',\n",
    "        capsize = 10 , elinewidth = 1, capthick = 1)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title('Win percentage against bot strategies')\n",
    "    plt.xlabel('Trial round')\n",
    "    plt.ylabel('Mean win percentage')\n",
    "    plt.axhline(y = 1/3, color = 'r', linestyle = '--')\n",
    "    \n",
    "    return g\n",
    "\n",
    "plot_win_rates(win_summary(groupby_f_data(df_3a,'agent_outcome',30),'agent_outcome'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
