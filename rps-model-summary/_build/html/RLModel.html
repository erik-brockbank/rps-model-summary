
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Model Overview &#8212; Modeling Adaptive Reasoning in ü™® üìú ‚úÇÔ∏è</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model Results" href="RLModel_results.html" />
    <link rel="prev" title="Reference: Model Code" href="ModelModel_code.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/rps_graphic.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Modeling Adaptive Reasoning in ü™® üìú ‚úÇÔ∏è</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="home.html">
                    Modeling Adaptive Reasoning in ü™® üìú ‚úÇÔ∏è
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Summary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Data.html">
   Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model-Based Agent
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ModelModel.html">
   Model Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelModel_results.html">
   Model Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelModel_code.html">
   Reference: Model Code
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model-Free RL Agent
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Model Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RLModel_results.html">
   Model Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RLModel_code.html">
   Reference: RL Model Code
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model Comparison
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ModelComparison.html">
   Model Comparison Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelComparison_results.html">
   Model Comparison Results
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Discussion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Discussion.html">
   Discussion
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  APPENDIX - code
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="model_python_lib_utils.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_utils.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_python_lib_event_counts.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_event_counts.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_python_lib_decision_functions.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_decision_functions.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_wrapper.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_wrapper.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RL_model_python_lib_utils.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     RL_model_utils.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RL_model_python_lib_reward.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     RL_model_reward.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RL_model_python_lib_decision_functions.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     RL_model_decision_functions.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_lib_visualization.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     visualization.py
    </span>
   </code>
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/RLModel.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model Overview
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-details">
   Model Details
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tracking-behavior-associated-rewards">
     Tracking behavior associated rewards
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-move">
     Choosing a move
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Model Overview</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model Overview
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-details">
   Model Details
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tracking-behavior-associated-rewards">
     Tracking behavior associated rewards
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-move">
     Choosing a move
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="model-overview">
<h1>Model Overview<a class="headerlink" href="#model-overview" title="Permalink to this headline">#</a></h1>
<p><em><strong>At a high-level, the RL-based agent simulates what would happen if (human) players were trying to determine their next optimal move on the basis of particular rewards associated with different past events (patterns in agent‚Äôs previous moves).</strong></em></p>
<p>In this way, the RL-based agent represents a kind of <em>idealized</em> model of human behavior that serves as a basis for evaluating what people are <em>actually</em> doing.</p>
<p>In order to simulate human play against the different bot opponents, the RL-based agent has two high-level functions (we spell these out in more detail below):</p>
<ol class="simple">
<li><p><strong>Track reward patterns</strong>: First, the agent maintains a count of <em>rewards</em> that it uses to try and predict its next move. To demonstrate, the simplest version is to straightly add up the rewards (3 for ‚ÄòWin‚Äô, 0 for ‚Äòtie‚Äô, -1 for ‚Äòlose) that the agent have from the previous rounds. In each round, these rewards allow for a (very rough) prediction that the agent will choose whichever move that generated a win on the previous round.</p></li>
<li><p><strong>Choose an optimal move</strong>: The RL-based choose the next move based on whichever move accumulates the highest rewards. The higher the reward is, the more likely the agent will choose this move. The agent use certain reward patterns we will define later to make its own move choice. In one word, tt samples a move each round probabilistically based on the reward of moves from the past.</p></li>
</ol>
<p><em><strong>How well does an RL-based model using the steps above capture human patterns of learning against the different bot opponents?</strong></em></p>
<p>To answer this question, we implement different versions of the model and compare them to the human behavior in our experiment data. Each version of the model has the same <em>decision function</em> from step 2 above but differs in the underlying information that it tracks about different combination of reward counts (step 1). In this way, we simulate different levels of <em>reward tracking</em> that people might be engaging in when playing against algorithmic opponents.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-details">
<h1>Model Details<a class="headerlink" href="#model-details" title="Permalink to this headline">#</a></h1>
<p>Below, we describe in more detail the two central aspects of the model-based agent‚Äôs behavior outlined above.</p>
<p><em>The content in this section is meant to provide enough detail for readers to inspect the underlying code or think carefully about the <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a> on the next page.</em></p>
<section id="tracking-behavior-associated-rewards">
<h2>Tracking behavior associated rewards<a class="headerlink" href="#tracking-behavior-associated-rewards" title="Permalink to this headline">#</a></h2>
<p>For an apples-to-apples comparison of model behavior to our experimental results, we run a version of the model alongside each human participant. Thus, model results are based on 300 simulated move choices in each of the games that human participants played; the agent‚Äôs decisions in these games is based on reward counts of various prior moves in each game, to ensure that the agent essentially <em>shared the same experience</em> as our human players.</p>
<p>The basis for the RL-based agent‚Äôs move choices is an ongoing count of patterns in previous moves. To compute this, we add columns to our existing data frame that maintain the relevant event counts from earlier rounds in each row. For example, the simplest version of the agent adds columns for <code class="docutils literal notranslate"><span class="pre">rock_reward</span></code>, <code class="docutils literal notranslate"><span class="pre">paper_reward</span></code>, and <code class="docutils literal notranslate"><span class="pre">scissors_reward</span></code>. Next, we iterate through each round of human play and update the reward based on the outcome the agent obtains by playing <em>Rock</em>, <em>Paper</em>, and <em>Scissors</em>. Since for this null model, we merely accumulate rewards of each single move, and the model favors whichever move (<code class="docutils literal notranslate"><span class="pre">Rock</span></code>, <code class="docutils literal notranslate"><span class="pre">Paper</span></code>, <code class="docutils literal notranslate"><span class="pre">Scissors</span></code>) wins the most.</p>
<p>In fact, the patterns that the RL-based agent tracks can be arbitrarily complex. In our <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a> on the next page, we evaluate a number of additional models that track different combinations of previous moves using the same process described above. These patterns fall into several distinct categories, described below in increasing complexity:</p>
<ul class="simple">
<li><p><strong>Self-previous move reward</strong>: This RL-based agent tracks the reward pattern of its own <em>previous move</em> and current move. This reward pattern allows agent to choose an optimal move given its previous move. In another word, if the agent play <code class="docutils literal notranslate"><span class="pre">Rock</span></code> last time, the agent is going to think which move gives me the highest reward after my <code class="docutils literal notranslate"><span class="pre">Rock</span></code> move. How to achieve this mindset? The model increments the current move‚Äôs reward based on the current outcome, ie. <em>win</em> (3 <span class="math notranslate nohighlight">\(points\)</span>), <em>tie</em> (0 <span class="math notranslate nohighlight">\(point\)</span>), and <em>loss</em> (-1 <span class="math notranslate nohighlight">\(point\)</span>). (For a reminder of how <em>reward counts</em> are formalized in rock, paper, scissors, see the latter <span class="xref myst">RL_model_reward</span> page). This pattern is well-aligned to the opponent-transition bot strategy; but <em>how</em> quickly the agent learns to choose their moves, how well it performs against the <em>other</em> bots, and whether this closely captures <em>human</em> learning against the bot opponents are all open questions (see <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a>).</p></li>
<li><p><strong>Opponent-previous move rewards</strong>: We also implement a version of the RL-based agent that tracks the <em>opponent‚Äôs previous move</em> rewards. This reward pattern allows agent to choose an optimal move given its opponent‚Äôs previous move. In another word, if the agent play <code class="docutils literal notranslate"><span class="pre">Paper</span></code> last time, the agent is going to think which move gives me the highest reward after my opponent plays <code class="docutils literal notranslate"><span class="pre">Paper</span></code>. How to know the agent‚Äôs rewards? For example, if the agent wins by playing <code class="docutils literal notranslate"><span class="pre">Rock</span></code> this time, the model increments the <code class="docutils literal notranslate"><span class="pre">Paper_Rock_reward</span></code> by 3; if the agent loses by play <code class="docutils literal notranslate"><span class="pre">Rock</span></code>, the model decrement the <code class="docutils literal notranslate"><span class="pre">Paper_Rock_reward</span></code> by 1. This pattern sounds like bing aligned to the self-transition bot strategy. It allows us to explore how well such reward pattern performs in simulated rounds against all the bot opponents, and whether this aligns with human learning. (see <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a>)</p></li>
<li><p><strong>Disjunctive agent-opponent previous move rewards</strong>: In addition to simple <em>self-</em> and <em>opponent-</em> previous move rewards, this RL-based agent tracks the disjunction of the opponent‚Äôs past move and human‚Äôs current move by picking the total higher-rewarded pattern first. To be clarified, the agent keeps both self-previous reward distribution and opponent-previous reward distribution in mind, and when it is going to choose a next move, it decides one favored pattern and pick a favored move from it. How does this selection works? For example, the bot plays <code class="docutils literal notranslate"><span class="pre">Rock</span></code> previously and the agent plays <code class="docutils literal notranslate"><span class="pre">Paper</span></code> previously. Now the agent perceives <code class="docutils literal notranslate"><span class="pre">Paper_{R,</span> <span class="pre">P,</span> <span class="pre">S}_rewards</span></code> as self-previous reward pattern (3 values), and <code class="docutils literal notranslate"><span class="pre">Rock_{R,</span> <span class="pre">P,</span> <span class="pre">S}_rewards</span></code> as opponent_previous reward pattern (3 values). Next, the agent sums up each pattern‚Äôs reward, and choose the higher total reward as the favored pattern. Then following the same procedure as before, the agent will pick a higher-rewarded move from the pattern. Using this strategy to pick move should perform better, because the agent is able to extract the winning information from one of the strategy.</p></li>
<li><p><strong>Combined agent-opponent previous move rewards</strong>: Finally, the most complex version of the model-based agent tracks the ongoing combined reward count of the opponent past move and human past move. This represents a further increase in complexity from the disjunctive version which only considers one-side information. As such, this level of reward count tracking requires 27 columns to maintain state from one round to the next. Given an example, this patter contains information as <code class="docutils literal notranslate"><span class="pre">{opponent_previous_move}_{human_previous_move}_{R,</span> <span class="pre">P,</span> <span class="pre">S}_reward</span></code>. While it is unlikely that human explicitly track previous moves on this level of complexity, this allows us to model ideal learning conditions for the most complex RL-agent and determine how such an agent performs by adopting simpler reward counting mechanisms.</p></li>
</ul>
</section>
<section id="choosing-a-move">
<h2>Choosing a move<a class="headerlink" href="#choosing-a-move" title="Permalink to this headline">#</a></h2>
<p>Each instantiation of the RL-based agent tracks a particular <em>reward count</em> or previous moves that it uses as the basis for choosing its optimal next move in a given round. <em>But how does the agent choose its own move on the basis of this information?</em></p>
<p><strong>Rather than being like the Model-based agent to calculate expected values of the move combination, the RL-based agent probabilistically choosing the move have the highest reward directly.</strong> What does this mean?</p>
<p>The RL-based agent converts the corresponding <code class="docutils literal notranslate"><span class="pre">Rock,</span> <span class="pre">Paper,</span> <span class="pre">and</span> <span class="pre">Scissors</span></code> reward columns to a probability distribution through a calculation of its <em>softmax probability</em> and sample a move from the distribution.</p>
<p><strong>Sample a move using <em>softmax</em></strong>: The RL-based agent has tracked rewards based on patterns we defined in different models. Given an example, <code class="docutils literal notranslate"><span class="pre">rock_rock_reward</span></code> in <code class="docutils literal notranslate"><span class="pre">agent_previous_move</span></code> model means human previous move is rock and current move is rock, and this column is from which the softmax probability of <code class="docutils literal notranslate"><span class="pre">Rock</span></code> comes. Knowing the prior move conditions, the agent grabs <code class="docutils literal notranslate"><span class="pre">{Prior_condition}_{Rock</span> <span class="pre">or</span> <span class="pre">Paper</span> <span class="pre">or</span> <span class="pre">Scissors}_reward</span></code> columns, and passes the values to the softmax probability calculation. The softmax formula is:</p>
<div class="math notranslate nohighlight">
\[
  P(M_a) = \dfrac{e^{\beta E[M_a]}}{\sum_{M_a \in \{R, P, S\}} e^{\beta E[M_a]}}
\]</div>
<p>Then it returns a probability distribution to sample <code class="docutils literal notranslate"><span class="pre">Rock</span></code>, <code class="docutils literal notranslate"><span class="pre">Paper</span></code> and <code class="docutils literal notranslate"><span class="pre">Scissors</span></code> in the current round.</p>
<p>Finally, the RL-model samples a move from the softmax probability distribution. In summary, the move with a higher rewards in a given round are more likely to be chosen. If the agent is successfully able to track the reward of the previous moves, it will have a high probability of choosing the move that <em>beats</em> the opponent‚Äôs next move.</p>
<p><em><strong>But how well does the RL-based agent, choosing its moves in simulated rounds through the process described above, perform against the bot opponents from our experiments?</strong></em></p>
<p>In the next page (<a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a>), we test this model‚Äôs performance in the context of different reward tracking pattern.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ModelModel_code.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Reference: Model Code</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="RLModel_results.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Results</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Betty Gong, MJ Mei, Annalea O'Halloran, Alison Yu, Erik Brockbank<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>