
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Model Overview &#8212; Modeling Adaptive Reasoning in ü™® üìú ‚úÇÔ∏è</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model Results" href="RLModel_results.html" />
    <link rel="prev" title="Reference: Model Code" href="ModelModel_code.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/rps_graphic.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Modeling Adaptive Reasoning in ü™® üìú ‚úÇÔ∏è</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="home.html">
                    Modeling Adaptive Reasoning in ü™® üìú ‚úÇÔ∏è
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Summary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Data.html">
   Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model-Based Agent
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ModelModel.html">
   Model Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelModel_results.html">
   Model Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelModel_code.html">
   Reference: Model Code
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model-Free RL Agent
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Model Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RLModel_results.html">
   Model Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RLModel_code.html">
   Reference: RL Model Code
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Discussion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Discussion.html">
   Discussion
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  APPENDIX - code
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="model_python_lib_utils.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_utils.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_python_lib_event_counts.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_event_counts.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_python_lib_decision_functions.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_decision_functions.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_wrapper.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     model_wrapper.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RL_model_python_lib_utils.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     RL_model_utils.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RL_model_python_lib_reward.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     RL_model_reward.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RL_model_python_lib_decision_functions.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     RL_model_decision_functions.py
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_lib_visualization.html">
   <code class="docutils literal notranslate">
    <span class="pre">
     visualization.py
    </span>
   </code>
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/RLModel.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model Overview
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-details">
   Model Details
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tracking-rewards-from-past-behavior">
     Tracking rewards from past behavior
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-move-to-maximize-expected-reward">
     Choosing a move to maximize expected reward
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Model Overview</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model Overview
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-details">
   Model Details
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tracking-rewards-from-past-behavior">
     Tracking rewards from past behavior
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-move-to-maximize-expected-reward">
     Choosing a move to maximize expected reward
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="model-overview">
<h1>Model Overview<a class="headerlink" href="#model-overview" title="Permalink to this headline">#</a></h1>
<p><em><strong>At a high-level, the Reinforcement Learning (RL) Agent simulates what would happen if human players were trying to determine their next move purely on the basis of what had been most successful previously.</strong></em></p>
<p>The RL Agent tracks the rewards (game points) associated with different combinations of its own moves and its opponent‚Äôs moves and chooses the action each round that it expects to be most successful based on this prior experience. While the <a class="reference internal" href="ModelModel.html"><span class="doc std std-doc">Model-Based Agent</span></a> tries to <em>predict</em> its opponent‚Äôs most likely next move, the RL Agent does not have any such prediction about its opponent; instead, it merely knows what move it expects to be most successful on the next round based on what it has seen so far.</p>
<p>In order to simulate human play against the different bot opponents, the RL Agent has two high-level functions (we spell these out in more detail below):</p>
<ol class="simple">
<li><p><strong>Track reward patterns</strong>: First, the agent maintains a count of <em>rewards</em> associated with different move combinations that it uses to determine which move patterns have been most successful. For example, the simplest version of the RL Agent might choose a move each round based on whichever of the three possible moves had generated the highest reward across all previous rounds. For such an agent, tracking reward patterns would involve adding up the total points (3 for each <em>win</em>, 0 for each <em>tie</em>, -1 for each <em>loss</em>) associated with each move choice (<code class="docutils literal notranslate"><span class="pre">Rock</span></code>, <code class="docutils literal notranslate"><span class="pre">Paper</span></code>, <code class="docutils literal notranslate"><span class="pre">Scissors</span></code>). This reward tracking allows the agent to choose whichever move has generated the greatest number of points so far. While this represents a very coarse means of deciding what move is likely to be successful, tracking rewards associated with richer combinations of previous moves allows this agent to respond more adaptively.</p></li>
<li><p><strong>Choose an optimal move</strong>: The RL Agent chooses its next move based on prior move sequences that have generated the highest rewards. The higher the accumulated rewards for a given move choice (given relevant previous events), the more likely the agent is to choose this move. Specifically, using the reward totals for each move that the agent could choose in a given round, the agent samples a move probabilistically based on these relative reward values.</p></li>
</ol>
<p><em><strong>How well does the RL Agent described above capture human patterns of learning against the different bot opponents?</strong></em></p>
<p>To answer this question, we implement different versions of the model and compare them to the human behavior in our experiment data, just as we did with the <a class="reference internal" href="ModelModel_results.html"><span class="doc std std-doc">Model-Based Agent analysis</span></a>. Each version of the RL Agent has the same <em>decision function</em> from step 2 above but differs in the underlying set of move combinations that it uses to maintain reward counts (step 1). In this way, we simulate different levels of <em>associative learning</em> that people might be engaging in when playing against algorithmic opponents.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-details">
<h1>Model Details<a class="headerlink" href="#model-details" title="Permalink to this headline">#</a></h1>
<p>Here, we describe in more detail the two central aspects of the RL Agent‚Äôs behavior outlined above.</p>
<p><em>The content in this section is meant to provide enough detail for readers to inspect the underlying code or think carefully about the <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a> on the next page.</em></p>
<p>Our model evaluation mirrors the <a class="reference internal" href="ModelModel_results.html"><span class="doc std std-doc">Model-Based Agent analysis</span></a>; we compare the RL Agent to human decision making by running simulations of the agent for each human participant in the original experiment. Thus, model results are based on 300 agent-supplied move choices in each of the games that human participants played; however, the RL Agent‚Äôs decisions in these games are based on reward counts of the previous <em>human and bot opponent</em> moves in each game. Critically, this means the agent‚Äôs move choices each round reflect learning from the participant behaviors, not learning from what the agent <em>would have done</em> in earlier rounds.</p>
<section id="tracking-rewards-from-past-behavior">
<h2>Tracking rewards from past behavior<a class="headerlink" href="#tracking-rewards-from-past-behavior" title="Permalink to this headline">#</a></h2>
<p>The basis for the RL Agent‚Äôs move choices is an ongoing count of rewards associated with previous move combinations. To compute this, we add columns to our existing data frame that maintain the relevant cumulative rewards for the agent from earlier rounds. For example, the simplest version of the RL Agent adds columns for <code class="docutils literal notranslate"><span class="pre">rock_reward</span></code>, <code class="docutils literal notranslate"><span class="pre">paper_reward</span></code>, and <code class="docutils literal notranslate"><span class="pre">scissors_reward</span></code>; these track the cumulative rewards associated with each move choice in prior rounds. To populate them, we iterate through each round of human play and update the reward values in these columns based on the participant‚Äôs move choice that round (<code class="docutils literal notranslate"><span class="pre">Rock</span></code>, <code class="docutils literal notranslate"><span class="pre">Paper</span></code>, or <code class="docutils literal notranslate"><span class="pre">Scissors</span></code>), and the outcome the participant obtained (3 points for a <em>win</em>, 0 points for a <em>tie</em>, -1 points for a <em>loss</em>). Since this simplified model merely accumulates rewards associated with each single move, the model favors whichever move has won the most. We use this as a null model, since none of the bot opponents in the experiment could be exploited by simply favoring a particular move.</p>
<p>However, the patterns that the RL Agent uses to track rewards aren‚Äôt restricted to move base rates and can be arbitrarily complex. In our <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a> on the next page, we evaluate a number of additional models that track different combinations of previous moves using the same process described above. These patterns fall into several distinct categories, described below:</p>
<ul class="simple">
<li><p><strong>Self previous move rewards</strong>: This version of the RL Agent tracks the rewards associated with each possible combination of its own previous move and current move. In other words, if the agent (or, more accurately, the human participant the agent is simulating) previously played <code class="docutils literal notranslate"><span class="pre">Rock</span></code>, its reward tracking allows it to choose the move that has the highest cumulative rewards following a choice of <code class="docutils literal notranslate"><span class="pre">Rock</span></code>. This strategy is well-aligned to the <em>opponent-transition bot</em> because, against this opponent, there will always be a move which performs best for the agent given a particular previous move; but <em>how</em> quickly the RL Agent learns this pattern when simulating human play against this bot and how well it performs against the <em>other</em> bots are all open questions (see <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a>).</p></li>
<li><p><strong>Opponent previous move rewards</strong>: We also implement a version of the RL Agent that tracks rewards associated with each combination of agent move and <em>opponent previous move</em>. Tracking this reward pattern allows the agent to choose the optimal move each round based on any dependency between its move choice and its opponent‚Äôs previous move. To illustrate, if the agent‚Äôs opponent played <code class="docutils literal notranslate"><span class="pre">Paper</span></code> in the previous round, the agent will tend to choose the move for which it has obtained the highest reward previously after the opponent played <code class="docutils literal notranslate"><span class="pre">Paper</span></code>. This pattern is well-adapted to the <em>self-transition bot</em>, since that bot will reliably choose a particular move after its own previous move. More generally, this version of the RL Agent allows us to explore how well <em>opponent previous move</em> reward tracking performs in simulated rounds against all the bot opponents, and whether this aligns with human learning (see <a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a>).</p></li>
<li><p><strong>Combined agent-opponent previous move rewards</strong>: In addition to simple <em>self-</em> and <em>opponent-</em> previous move rewards, this version of the RL Agent tracks the rewards achieved through a <em>combination</em> of the opponent‚Äôs previous move and the agent‚Äôs own previous move. In other words, if the previous round was a tie in which the RL agent and its opponent both chose <code class="docutils literal notranslate"><span class="pre">Paper</span></code>, this version of the RL Agent will prefer whichever move had the highest combined rewards in previous rounds in which both players‚Äô previous moves were <code class="docutils literal notranslate"><span class="pre">Paper</span></code>. Under the hood, this agent requires 27 columns to maintain reward counts from one round to the next, one for each unique combination of agent previous move, opponent previous move, and potential subsequent move. This makes it adaptive against all but the most complex bot opponent, since the bot opponents‚Äô move choices are by and large determined by a subset or combination of their own and the human participant‚Äôs previous move.</p></li>
<li><p><strong>Disjunctive agent-opponent previous move rewards</strong>: A final version of the RL agent represents a middle ground between the self-/opponent- previous move rewards agent and the combined previous move agent above. This agent tracks the cumulative rewards associated with each combination of its own previous move and subsequent move, as well as the rewards associated with each combination of its opponent‚Äôs previous move and its own subsequent move. However, rather than tracking each unique set of previous moves like the combined previous move agent above, this version tracks each previous move dependency <em>on its own</em>. When it comes time to choose a move, it combines the rewards for each possible move given the previous opponent move and rewards for each possible move given its own previous move and uses these combined values to select a move. Thus, in the scenario described above where the agent and its bot opponent tied in the previous round by each choosing <code class="docutils literal notranslate"><span class="pre">Paper</span></code>, this version of the RL Agent will tend to choose the move which has the highest reward associated with <em>either</em> the agent‚Äôs own previous move of <code class="docutils literal notranslate"><span class="pre">Paper</span></code> <em>or</em> its opponent‚Äôs previous move of <code class="docutils literal notranslate"><span class="pre">Paper</span></code>, rather than the move which was historically best when <em>both</em> players previously chose <code class="docutils literal notranslate"><span class="pre">Paper</span></code>. In this way, it is an intermediate stage between the self and opponent previous move agents and the combined previous move agent because it requires 18 columns of cumulative reward counts, nine for each combination of self previous move and subsequent move, and nine for each combination of opponent previous move and agent move. Because the agent doesn‚Äôt integrate both previous moves in the same way as the combined previous move agent above, it is not as well adapted to the bots whose move choices are based on previous round outcomes (since this is a contingency on both previous moves combined).</p></li>
</ul>
<p><strong>Taken together, the RL Agent‚Äôs reward tracking process involves updating cumulative reward values for each move choice given distinct combinations of previous moves by the agent and its opponent.</strong> The RL Agent uses these cumulative rewards to <em>learn</em> associations between its opponent‚Äôs or its own previous move and the optimal subsequent move. The different classes of RL Agent described above allow us to test distinct learning models that might best account for human behavior against the bots.</p>
<p><em>In the next section, we walk through how the RL Agent uses the signal from accumulated rewards to select optimal moves in simulated rounds against the  bot opponents.</em></p>
</section>
<section id="choosing-a-move-to-maximize-expected-reward">
<h2>Choosing a move to maximize expected reward<a class="headerlink" href="#choosing-a-move-to-maximize-expected-reward" title="Permalink to this headline">#</a></h2>
<p>Each instantiation of the RL Agent tracks a particular set of rewards based on one of the previous move patterns described above. <em>But how does the agent choose its move each round using this information?</em></p>
<p><strong>Unlike the <a class="reference internal" href="ModelModel.html"><span class="doc std std-doc">Model-Based Agent</span></a>, which chooses moves based on underlying predictions of what the opponent will do next, the RL Agent merely chooses moves in proportion to the rewards they conferred in the past.</strong></p>
<p>In each round, the RL agent has a cumulative reward associated with choosing <code class="docutils literal notranslate"><span class="pre">Rock</span></code>, <code class="docutils literal notranslate"><span class="pre">Paper</span></code>, and <code class="docutils literal notranslate"><span class="pre">Scissors</span></code> in previous rounds; as described above, these rewards incorporate additional <em>contingencies</em> like the participant‚Äôs previous move or their bot opponent‚Äôs previous move (or both). The task for the RL agent is to sample moves that are associated with the largest prior rewards (given any relevant contingency information). To do this, the agent first converts these reward values to a probability distribution by calculating each move‚Äôs <em>softmax probability</em>, and then samples a move from the resulting softmax distribution.</p>
<p><strong>Sample a move based on its <em>softmax probability</em></strong>: For the RL Agent, its move each round will be chosen based on the relative prior rewards associated with each possible choice. For example, if the <em>self-previous move</em> agent above has chosen <code class="docutils literal notranslate"><span class="pre">Rock</span></code> in the previous round, it will first select the cumulative rewards associated with each possible move choice after <code class="docutils literal notranslate"><span class="pre">Rock</span></code>. Intuitively, if playing <code class="docutils literal notranslate"><span class="pre">Paper</span></code> after <code class="docutils literal notranslate"><span class="pre">Rock</span></code> has resulted in much higher rewards in the past than playing <code class="docutils literal notranslate"><span class="pre">Scissors</span></code> or <code class="docutils literal notranslate"><span class="pre">Rock</span></code> after <code class="docutils literal notranslate"><span class="pre">Rock</span></code>, it should strongly favor <code class="docutils literal notranslate"><span class="pre">Paper</span></code>. On the other hand, if all three moves have produced roughly equivalent outcomes, this agent will be more indifferent about its move choice. To formalize this, the RL Agent computes a softmax probability distribution over the rewards for each possible move. For a candidate agent move <span class="math notranslate nohighlight">\(M_a\)</span> (e.g. <code class="docutils literal notranslate"><span class="pre">Paper</span></code>), the softmax probability of choosing that move is:</p>
<div class="math notranslate nohighlight">
\[
  P(M_a) = \dfrac{e^{\beta U(M_a)}}{\sum_{M_{a'} \in \{R, P, S\}} e^{\beta U(M_{a'})}}
\]</div>
<p>In the above formulation, <span class="math notranslate nohighlight">\(U(M_a)\)</span> represents the <em>utility</em> of choosing a move <span class="math notranslate nohighlight">\(M_a\)</span>‚Äîthis will simply be the prior rewards associated with that choice, given all relevant information (such as the agent‚Äôs own previous move). The probability of a move choice <span class="math notranslate nohighlight">\(M_a\)</span> is this value scaled by the combined prior rewards of all other possible moves <span class="math notranslate nohighlight">\(M_{a'}\)</span>. The <span class="math notranslate nohighlight">\(\beta\)</span> term in the numerator and denominator is a common parameter used to scale <em>how much the agent should favor the highest utility move</em>. In this version of the model, we simply set this to 1. However, in another line of work we estimate a <em>fitted <span class="math notranslate nohighlight">\(\beta\)</span> parameter</em> for each participant as a way of estimating how well the RL Agent describes human move decisions.</p>
<p>The RL Agent uses the softmax tranformation above to generate a probability distribution over possible moves <span class="math notranslate nohighlight">\(M_a \in \{R, P, S\}\)</span>. To select a move, the RL Agent samples a move from this softmax probability distribution. Note that while the <a class="reference internal" href="ModelModel_results.html"><span class="doc std std-doc">Model-Based Agent</span></a> <em>also</em> relied on a softmax distribution to choose its move, the underlying basis for the distribution was different. In that case, the agent chose its moves in proportion to their underlying <em>expected value</em>. Here, the agent instead chooses a move in proportion to its <em>previous rewards</em>.</p>
<p>In summary, moves with higher rewards from previous rounds are more likely to be chosen in subsequent rounds. This means that if the agent is successfully able to track patterns in its own or its opponent‚Äôs previous moves that systematically lead to higher rewards, it will have a high probability of choosing the move that <em>beats</em> the opponent‚Äôs next move.</p>
<p><em><strong>But how well does the RL Agent, choosing its moves in simulated rounds through the process described above, perform against the bot opponents from our experiments?</strong></em></p>
<p>In the next page (<a class="reference internal" href="RLModel_results.html"><span class="doc std std-doc">Model Results</span></a>), we test the RL Agent‚Äôs performance in the context of different reward tracking patterns.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ModelModel_code.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Reference: Model Code</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="RLModel_results.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Results</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Betty Gong, MJ Mei, Annalea O'Halloran, Alison Yu, Erik Brockbank<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>